### Scraping web data behind web forms

The most difficult scenario for web scraping is when data is hidden behind multiple pages that can only be accessed entering information into web forms. There are a few approaches that might work in these cases, with varying degree of difficulty and reliability, but in my experience the best method is to use [Selenium](https://en.wikipedia.org/wiki/Selenium_(software)).

Selenium automates web browsing sessions, and was originally designed for testing purposes. You can simulate clicks, enter information into web forms, add some waiting time between clicks, etc. To learn how it works, we will scrape a heavily javascripted website of 2017 General Election results. (You can download the information from the governemnt websites, but well, this is an example.)

```{r}
url <- 'https://www.theguardian.com/politics/ng-interactive/2017/jun/08/live-uk-election-results-in-full-2017'
```

As you can see, the information we want to scrape is dynamically displayed by putting information in the search field. By checking the website source, you can confirm that the information is not in the html but rendered dynamically when you select a particular url.

The first step is to load the RSelenium. Then, we will start a browser running in the background. I will use Google Chrome, but Firefox should work.

```{r}
library(RSelenium)
library(tidyverse)
library(stringi)
library(rvest)
library(xml2)
```


Start selenium server in chrome
```{r}
driver<- rsDriver(browser=c("chrome"), port = 4568L)
browser <- driver[["client"]]
browser$navigate(url)
```

Here's how we would check that it worked:

```{r}
src <- browser$getPageSource()
substr(src, 1, 1000)
```

First thing first, the following code will remove the cookie banner at the bottom.

```{r}
cookie_button <- browser$findElement(using = 'css selector', value = "#top > div.site-message.js-site-message.site-message--banner.site-message--first-pv-consent.site-message--permanent > div > div > div.site-message__copy.js-site-message-copy.u-cf > div.site-message--first-pv-consent__actions > button")
cookie_button$clickElement()
```

First, the search element has to be shown in the screen. So, let scroll. 

```{r}
webElem <- browser$findElement("css", "body")
webElem$sendKeysToElement(list(key = "space"))
```

Let's assume we want to see the results of the constituency here. We can feed post code, and check the results.  First, let's use selectorGadget to identify the elements that we're trying to scrape. Then, send the text to the field and "enter" key inputs.

```{R}
## identify the ndoe for input
search_field <- browser$findElement(using = 'class name', value = 'ge-lookup__input')
## send the post code
search_field$sendKeysToElement(list("WC2A 2AE"))
## This is a tricky part, we need to wait until a suggestion shows up
while(browser$findElement(using = 'class name', value = 'ge-lookup__suggestions')$getElementText() %>% nchar() == 0) 
  Sys.sleep(1)

## mock the click enter
search_field$sendKeysToElement(list(key = "enter"))
```

Now that we have the results table displayed, we will scrape the name of constituency and the table.

```{r}
## get the constituency name
const_name <- browser$findElement(using = 'class name', value = 'ge-result__name')$getElementText() 

## get the div with the result information
res_div <- browser$findElement(using = 'class name', value = 'ge-result')

## what we can do here is identify the root node where the results are displayed
## and then you can hand the html from browser to 
## rvest and use familer html_table() function
## get the html of the table, then parse it using rvest's "html_table"
results_html <- read_html(res_div$getElementAttribute('innerHTML')[[1]])
results_table <- html_table(results_html)[[1]]
names(results_table)[c(1, 5)] <- c('tmp', 'tmp2')
```

The first column of the table was supposedly the party. But that information is not coming through, because it's just blank `<td>` tags. We still can extract the information by using the class information attached. 

```{r}
# the code here finds a span in the first column of a table row, and extract the 
# value of class attributes
party_class <- results_html %>% html_nodes(xpath = "//tr/td[1]/span") %>%
  html_attr("class")
print(party_class)
# there are some extra texts here. Remove them using `stri_replace_*` function.
# 
party <- stri_replace_first_regex(party_class, ".+-", "")
print(party)
```

Now, let's create a `data.frame`.

```{R}
results_table <- results_table %>%
  mutate(constituency = const_name) %>% # create a new variable to get the constituency name
  mutate(party = party) %>% # create a new variable for party names
  mutate(votes = stri_replace_all_regex(votes, "\\D+", "") %>% as.numeric) %>% # make the vote count numeric
  select(c(2, 3, 6, 7)) # select columns to keep

print(results_table)
```


We think that we have identified the necessary steps to get the data. We can now go over the list of constituency names and get all candidate data.

First, enerate a function to search with constituency name and get the table
```{r}
get_results_by_const <- function(const_name, sec = 4){
  url <- 'https://www.theguardian.com/politics/ng-interactive/2017/jun/08/live-uk-election-results-in-full-2017'
  # scroll the page
  webElem <- browser$findElement("css", "body")
  webElem$sendKeysToElement(list(key = "space"))
  search_field <- browser$findElement(using = 'class name', value = 'ge-lookup__input')
  search_field$sendKeysToElement(list(const_name))
  while(browser$findElement(using = 'class name', value = 'ge-lookup__suggestions')$getElementText() %>% nchar() == 0) {
    Sys.sleep(1)
  }

  search_field$sendKeysToElement(list(key = "enter"))
  
  ## get the div with the result information
  res_div <- browser$findElement(using = 'class name', value = 'ge-result')
  ## get the html of the table, then parse it using rvest's "html_table"
  results_html <- read_html(res_div$getElementAttribute('innerHTML')[[1]])
  results_table <- html_table(results_html)[[1]]
  names(results_table)[c(1, 5)] <- c('tmp', 'tmp2')
  party_class <- results_html %>% html_nodes(xpath = "//tr/td[1]/span") %>%
    html_attr("class")
  party <- sub(".+-", "", party_class)
  results_table <- results_table %>%
    mutate(constituency = const_name) %>%
    mutate(party = party) %>%
    mutate(votes = stri_replace_all_regex(votes, "\\D", "") %>% as.numeric)
  Sys.sleep(sec)
  return(results_table)
}

```

Now, we get the name of constituencies from an excel file.

```{R}
library(readxl)
const_data <- read_xls("SAPE20DT7-mid-2017-parlicon-syoa-estimates-unformatted.xls", sheet = 4, skip = 4)
# replace "and" with "&"
const_data <- const_data %>% 
  mutate(PCON11NM = stri_replace_all_regex(PCON11NM, "\\band\\b", "&"))
```

Run the loop. 

Get first ten of them:
```{R}
data_all <- lapply(const_data$PCON11NM[1:10], get_results_by_const) %>% bind_rows()
head(data_all)
```

