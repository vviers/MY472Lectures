---
title: "Scraping newspaper RSS - Answer Key"
author: "Pablo Barbera and Akitaka Matsuo"
date: November 1, 2018
output: html_document
---

In this problem set, we will scrape the home page of [The Guardian](www.theguardian.com). Combining the techniques we have covered in the class so far, the goal is to produce a .csv dataset with the URL of each article that appears in the home page, its headline, and the text of the article.

Let's read through the [Guardian's RSS documentation](https://www.theguardian.com/help/feeds). As you can see, RSS is provided for each of the news category and the url is always https://www.theguardian.com/####/rss. You can even find some sub category of specific news category. For instance, the following works: [https://www.theguardian.com/world/japan/rss](https://www.theguardian.com/world/japan/rss). You can use what you are interested in. 

As an example, we are going to use Brexit.

```{r}
url <- "https://www.theguardian.com/politics/eu-referendum/rss"
```

Before scraping, we can check the strcture of xml on the browser. What's the node for each news article?

We now get the xml for rss, using `xml2::read_xml()` .
```{r}
library(xml2)
library(rvest)
library(lubridate)
library(stringi)
library(readr)
rss_xml <- read_xml(url)

```


```{r}
## Identify Item ndoes
nodes <- xml_nodes(rss_xml, css = "item")
length(nodes)

## extract titles
title <- nodes %>% xml_node("title") %>% xml_text()
## extract description
description <- nodes %>% xml_node("description") %>% xml_text() %>% 
  stri_replace_all_regex("<.+?>", " ") # just remove tags
  
## extract datetime
datetime <- nodes %>% xml_node(xpath = "//dc:date") %>% 
  xml_text() %>%
  parse_datetime() 

## extract url
article_url <- nodes %>% xml_node(css = "guid") %>% 
  xml_text()

## combine them as a dataframe
data_guardian_articles <- data.frame(title, description, datetime, 
                                    article_url, 
                                    text = NA,
                                    stringsAsFactors = FALSE)
```
By now you should have a data frame that has the headline and the URL for each article as the two variables. Let's prototype how you could scrape the text in the body of each of those URLs. Pick the first URL and write some code to get an object (let's call it 'text') that contains the text of the article.


Let's try the first url. Find the element that contains the body of the article, and extract it.

```{r}
test_url <- article_url[1]
article_html <- read_html(test_url)
c_text <- article_html %>% 
  html_nodes(css = ".content__article-body p") %>% 
  html_text() %>%
  paste(collapse = "\n")

```

Now that the code works, write a loop that will generalize it to all the URLs in the homepage. Make sure you first create an empty variable in the dataframe (again, let's call it 'text', and that each iteration of the loop fills in the ith element of that vector with the text of the article.

```{r}
text_extractor <- function(c_url, sec = 3){
  article_html <- read_html(c_url)
  ## define the text processing here
  c_text <- article_html %>% html_nodes(css = ".content__article-body p") %>%
    html_text() %>%
    paste(collapse = "\n")
  
  ## the following line defines the duration of break between scraping
  Sys.sleep(sec)
  return(c_text)
}

article_contents <- lapply(article_url[1:3], text_extractor) 

```

