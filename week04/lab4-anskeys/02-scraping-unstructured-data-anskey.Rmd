---
title: "Scraping unstructured data"
author: "Pablo Barbera and Akitaka Matsuo"
date: October 25, 2018
output: html_document
---

### Scraping web data in unstructured format

A common scenario for web scraping is when the data we want is available in plain html, but in different parts of the web, and not in a table format. In this scenario, we will need to find a way to extract each element, and then put it together into a data frame manually.

The motivating example here will be the website `ipaidabribe.com`, which contains a database of self-reports of bribes in India. We want to learn how much people were asked to pay for different services, and by which departments.

```{r}
url <- 'http://ipaidabribe.com/reports/paid'
```

We will also be using `rvest`, but in a slightly different way: prior to scraping, we need to identify the CSS selector of each element we want to extract. 

A very useful tool for this purpose is `selectorGadget`, an extension to the Google Chrome browser. Go to the following website to install it: `http://selectorgadget.com/`. Now, go back to the ipaidabribe website and open the extension. Then, click on the element you want to extract, and then on the rest of highlighted elements that you do __not__ want to extract. After only the elements you're interested in are highlighted, copy and paste the CSS selector into R.

Now we're ready to scrape the website:

```{r}
library(rvest, warn.conflicts=FALSE)
bribes <- read_html(url) # reading the HTML code
amounts <- html_nodes(bribes, ".paid-amount span") # identify the CSS selector
amounts # content of CSS selector
html_text(amounts)
```

We still need to do some cleaning before the data is usable:

```{r}
amounts <- html_text(amounts)
# remove text, white space, and commas
amounts <- gsub("Paid INR ", "", amounts) 
amounts <- gsub(" |\r|\n|", "", amounts) # remove white space
amounts <- gsub(",", "", amounts) # remove commas
(amounts <- as.numeric(amounts)) # convert to numeric
```

Let's do another one: transactions during which the bribe ocurred
```{r}
transaction <- html_nodes(bribes, ".transaction a")
(transaction <- html_text(transaction))
```

And one more: the department that is responsible for these transactions
```{r}
# and one more
dept <- html_nodes(bribes, ".name a")
(dept <- html_text(dept))
```

This was just for one page, but note that there are many pages. How do we scrape the rest? First, following the best practices on coding, we will write a function that takes the URL of each page, scrapes it, and returns the information we want.

```{r}
scrape_bribe <- function(url){
	bribes <- read_html(url)
	# variables that we're interested in
	amounts <- html_text(html_nodes(bribes, ".paid-amount span"))
  amounts <- gsub("Paid INR ", "", amounts) 
  amounts <- gsub(" |\r|\n|", "", amounts) # remove white space
  amounts <- gsub(",", "", amounts) # remove commas
  amounts <- as.numeric(amounts) # convert to numeric
	amounts <- as.numeric(gsub("Paid INR | |\r|\n|,", "", amounts))
	transaction <- html_text(html_nodes(bribes, ".transaction a"))
	dept <- html_text(html_nodes(bribes, ".name a"))
	# putting together into a data frame
	df <- data.frame(
		amounts = amounts,
		transaction = transaction,
		dept = dept,
			stringsAsFactors=F)
	return(df)
}
```

And we will start a list of data frames, and put the data frame for the initial page in the first position of that list.

```{r}
## create an empty list, where we will put the data
bribe_data_list <- list()
## the first element of this list is the first page
bribe_data_list[[1]] <- scrape_bribe(url)
str(bribe_data_list)
```

How should we go about the following pages? Note that the following urls had `page=XX`, where `XX` is 10, 20, 30... So we will create a base url and then add these additional numbers. (Note that for this exercise we will only scrape the first 5 pages.)

```{r}
## this is the base url which will be combined with the sequential numbers defined by the following line
base_url <- "http://ipaidabribe.com/reports/paid?page="
## we will scrape 4 pages
pages <- seq(0, 40, by=10)
```

And now we just need to loop over pages, and use the function we created earlier to scrape the information, and add it to the list. Note that we're adding a couple of seconds between HTTP requests to avoid overloading the page, as well as a message that will informs us of the progress of the loop.

```{r}

# the loop is defined over the `pages` sequence defined above
# the first element is skipped because we have already scraped it
for (i in 2:length(pages)){
	# informative message about progress of loop
	message(i, '/', length(pages))
	# prepare URL, using "paste0" command, we will construct a url
	url <- paste0(base_url, pages[i])
	# scrape website, and put in the list
	bribe_data_list[[i]] <- scrape_bribe(url)
	# wait a couple of seconds between URL calls
	Sys.sleep(2)
}
```

The final step is to convert the list of data frames into a single data frame that we can work with, using the function `do.call(rbind, LIST)` (where `LIST` is a list of data frames). 

```{r}
bribe_data <- do.call(rbind, bribe_data_list)
head(bribe_data)
str(bribe_data)
```

Let's get some quick descriptive statistics to check everything worked. First, what is the most common transaction during which a bribe was paid?

```{r}
tab <- table(bribe_data$transaction) # frequency table
tab <- sort(tab, decreasing=TRUE)	# sorting the table from most to least common
head(tab)
```

What was the average bribe payment?

```{r}
summary(bribe_data$amount)
```

And what was the average payment for each department?
```{r}
agg <- aggregate(bribe_data$amount, by=list(dept=bribe_data$dept), FUN=mean)
agg[order(agg$x, decreasing = TRUE),] # ordering from highest to lowest
```




